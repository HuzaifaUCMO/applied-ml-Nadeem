{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "title",
      "metadata": {},
      "source": [
       "# Lab 4 – Titanic Fare Regression\n",
       "**Name:** Huzaifa Nadeem  \n",
       "**Date:** 3‑21‑2025\n",
       "\n",
       "We move from classification to **regression**, predicting the continuous variable **`fare`** in the Titanic dataset, then compare Linear, Ridge, Elastic Net, and Polynomial models; a bonus section applies the workflow to Iris."
      ]
     },
     {
      "cell_type": "markdown",
      "id": "imports_hdr",
      "metadata": {},
      "source": [
       "## Imports"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
       "import seaborn as sns, pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
       "from sklearn.model_selection import train_test_split\n",
       "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n",
       "from sklearn.preprocessing import PolynomialFeatures\n",
       "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "sec1",
      "metadata": {},
      "source": [
       "# 1  Import & Inspect Data"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "load",
      "metadata": {},
      "outputs": [],
      "source": [
       "titanic = sns.load_dataset('titanic')\n",
       "print(titanic.shape)\n",
       "titanic.head()"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "sec2",
      "metadata": {},
      "source": [
       "# 2  Data Exploration & Preparation"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "prep",
      "metadata": {},
      "outputs": [],
      "source": [
       "# fill age without chained‑assignment warning\n",
       "titanic['age'] = titanic['age'].fillna(titanic['age'].median())\n",
       "\n",
       "# drop rows with missing fare\n",
       "titanic = titanic.dropna(subset=['fare'])\n",
       "\n",
       "# feature engineering\n",
       "titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1\n",
       "titanic['sex'] = titanic['sex'].map({'male':0,'female':1})\n",
       "titanic['embarked'] = titanic['embarked'].map({'C':0,'Q':1,'S':2})"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "sec3",
      "metadata": {},
      "source": [
       "# 3  Feature Selection (4 Cases)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "cases",
      "metadata": {},
      "outputs": [],
      "source": [
       "X1, y1 = titanic[['age']], titanic['fare']\n",
       "X2, y2 = titanic[['family_size']], titanic['fare']\n",
       "X3, y3 = titanic[['age','family_size']], titanic['fare']\n",
       "X4, y4 = titanic[['pclass','sex','family_size']], titanic['fare']"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "reflect3",
      "metadata": {},
      "source": [
       "**Why might these features affect a passenger’s fare?** Higher class, gender norms, age, and family size all influenced ticket prices in 1912.  \n",
       "**List all available features:** pclass, sex, age, sibsp, parch, fare, embarked, class, who, deck, embark_town, alive, alone, family_size.  \n",
       "**Which other features could improve predictions and why?** `deck` or `embark_town` might capture cabin quality or port‑based price differences.  \n",
       "**How many variables are in your Case 4?** Three.  \n",
       "**Which variables did you choose for Case 4 and why?** `pclass`, `sex`, and `family_size` because they correlate strongly with fare and capture socioeconomic and group‑size effects."
      ]
     },
     {
      "cell_type": "markdown",
      "id": "sec4",
      "metadata": {},
      "source": [
       "# 4  Linear Regression on All 4 Cases"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "linreg",
      "metadata": {},
      "outputs": [],
      "source": [
       "def split(X,y):\n",
       "    return train_test_split(X,y,test_size=0.2,random_state=123)\n",
       "\n",
       "splits = [split(X1,y1), split(X2,y2), split(X3,y3), split(X4,y4)]\n",
       "models, metrics = [], []\n",
       "\n",
       "for Xtr,Xte,ytr,yte in splits:\n",
       "    m = LinearRegression().fit(Xtr,ytr)\n",
       "    ytr_pred, yte_pred = m.predict(Xtr), m.predict(Xte)\n",
       "    rmse = np.sqrt(mean_squared_error(yte, yte_pred))\n",
       "    mae  = mean_absolute_error(yte, yte_pred)\n",
       "    models.append(m)\n",
       "    metrics.append([r2_score(ytr,ytr_pred), r2_score(yte,yte_pred), rmse, mae])\n",
       "\n",
       "metric_df = pd.DataFrame(metrics,columns=['Train R²','Test R²','RMSE','MAE'],index=['Case1','Case2','Case3','Case4'])\n",
       "metric_df"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "reflect4",
      "metadata": {},
      "source": [
       "**Did Case 1 over‑fit or under‑fit?** Under‑fit; age alone misses much variance.  \n",
       "**Did Case 2 over‑fit or under‑fit?** Under‑fit with poorest R².  \n",
       "**Did Case 3 over‑fit or under‑fit?** Better than Case 1, still modest.  \n",
       "**Did Case 4 over‑fit or under‑fit?** Best generalization with balanced train/test scores.  \n",
       "**Did adding age improve the model?** Yes; Case 3 beat Case 2 by capturing fare differences at extreme ages.  \n",
       "**Which case performed the worst and why?** Case 2—family_size alone weakly correlates with fare.  \n",
       "**Which case performed the best and why?** Case 4—multi‑feature mix captured socioeconomic and group effects."
      ]
     },
     {
      "cell_type": "markdown",
      "id": "sec5",
      "metadata": {},
      "source": [
       "# 5  Alternative Models on Best Case (Case 4)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "alt_models",
      "metadata": {},
      "outputs": [],
      "source": [
       "Xtr,Xte,ytr,yte = splits[3]\n",
       "\n",
       "ridge   = Ridge(alpha=1.0).fit(Xtr,ytr)\n",
       "elastic = ElasticNet(alpha=0.3,l1_ratio=0.5).fit(Xtr,ytr)\n",
       "\n",
       "poly = PolynomialFeatures(degree=3)\n",
       "Xtr_p = poly.fit_transform(titanic[['age']].iloc[ytr.index])\n",
       "Xte_p = poly.transform(titanic[['age']].iloc[yte.index])\n",
       "poly_mod = LinearRegression().fit(Xtr_p,ytr)\n",
       "\n",
       "def metrics(yhat):\n",
       "    return [r2_score(yte,yhat), np.sqrt(mean_squared_error(yte,yhat)), mean_absolute_error(yte,yhat)]\n",
       "\n",
       "comp = pd.DataFrame([\n",
       "    ['Linear']     + metrics(models[3].predict(Xte)),\n",
       "    ['Ridge']      + metrics(ridge.predict(Xte)),\n",
       "    ['ElasticNet'] + metrics(elastic.predict(Xte)),\n",
       "    ['Polynomial'] + metrics(poly_mod.predict(Xte_p))\n",
       "],columns=['Model','Test R²','RMSE','MAE'])\n",
       "comp"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "poly_plot",
      "metadata": {},
      "source": [
       "## 5.1 Plot Polynomial Fit (age → fare)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "poly_vis",
      "metadata": {},
      "outputs": [],
      "source": [
       "plt.scatter(Xte_p[:,1], yte, color='blue', label='Actual')\n",
       "plt.scatter(Xte_p[:,1], poly_mod.predict(Xte_p), color='red', label='Cubic Pred')\n",
       "plt.legend(); plt.xlabel('Age'); plt.ylabel('Fare'); plt.title('Cubic Polynomial Fit'); plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "reflect5",
      "metadata": {},
      "source": [
       "**How well did each model perform?** Ridge > Linear > ElasticNet > Polynomial on Test R².  \n",
       "**Any surprising results?** Cubic polynomial over‑fit older ages and hurt overall accuracy.  \n",
       "**Why might Ridge outperform others?** Its L2 penalty shrinks extreme coefficients without discarding useful features."
      ]
     },
     {
      "cell_type": "markdown",
      "id": "sec6",
      "metadata": {},
      "source": [
       "# 6  Final Thoughts & Insights\n",
       "- pclass, sex, and family_size were strongest predictors.  \n",
       "- Ridge regression delivered the best balance of bias/variance.  \n",
       "- Regularization helped; high‑degree polynomials hurt generalization."
      ]
     },
     {
      "cell_type": "markdown",
      "id": "bonus_hdr",
      "metadata": {},
      "source": [
       "---\n",
       "## Bonus – Iris Petal Length Regression"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "bonus",
      "metadata": {},
      "outputs": [],
      "source": [
       "iris = sns.load_dataset('iris')\n",
       "Xb = iris[['sepal_length','sepal_width','petal_width']]\n",
       "yb = iris['petal_length']\n",
       "Xtr_b,Xte_b,ytr_b,yte_b = train_test_split(Xb,yb,test_size=0.2,random_state=1)\n",
       "ridge_b = Ridge().fit(Xtr_b,ytr_b)\n",
       "print('Iris Ridge R²:', r2_score(yte_b,ridge_b.predict(Xte_b)))"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   